{
  "number": 9,
  "title": "Recognition as a Classification Problem",
  "sections": [
    {
      "title": "Why State Spaces?",
      "colorScheme": "dark",
      "subsections": [
        {
          "title": "Subsection 1",
          "body": "Often times, when we want to learn about a phenomena, whether that be the weather, sports, or business sales, we collect large amounts of data to extract information from. We collect this data about the specific characteristics that we can directly observe and write down, generally we collect two types of data: qualitative and quantitative (numerical) data. Today our discussion will center around quantitative data.",
          "imgSrc": "blank"
        },
        {
          "title": "Subsection 2",
          "body": "Let's pretend we are florists and we want to learn about the characteristics of the flowers in our garden. We know we have three types of Iris flowers in our garden: Setosa, Versicolor, and Virginica flowers. We go through and collect some easily measurable information about all of the flowers, particularly, we collect information about the petal length and width as well as the sepal length and width, keeping of which Iris variety these measurements came from.",
          "imgSrc": "blank"
        },
        {
          "title": "Subsection 3",
          "body": "Fantastic! Now we have the data for every single one of our plants, which we can put into a table. Using the lingo of classification, each raw of the table is an 'observation', each column is a 'feature', and the variable we want to predict is the 'target'. In our example, the target is the Iris species. The possible values of the target - e.g. Setosa, Virginica, and Versicolor - are called the 'classes'. The goal of classification is to correctly determine the target value (or the class) of each observation based upon its features. Can you come up with a way to determine the flower's species just based on the table below? Note that you can click the triangle in the top right corner of the table to reveal the observation's target variable. ",
          "imgSrc": "blank"
        }
      ],
      "demoComp": "RawDataTable"
    },
    {
      "title": "Projecting Data",
      "colorScheme": "light",
      "subsections": [
        {
          "title": "Subsection 1",
          "body": "It is certainly not easy to determine which observation is which flower just from the data alone! Another approach to understanding our data is to plot the features of our observations against each other to see if there's some correlation between their values and the target variable.",
          "imgSrc": "blank"
        },
        {
          "title": "Subsection 2",
          "body": "A straightforward approach to this is to project our data into a 2D plot. Here we are plotting the values for individual features along each axis. So for a single data point, the x axis represents the Petal Length of the flower and the y axis represents the Petal Width of the flower. So a single flower can be represented as (Petal Length, Petal Width).",
          "imgSrc": "blank"
        },
        {
          "title": "Subsection 3",
          "body": "First, let's only compare the Setosa and Versicolor flowers. Since we remembered to write down what subtype each of our flowers/observations is in our current garden, we can color our points based upon that information. Here, the Setosa flowers are blue and the Versicolor flowers are red. In our graph, we can see a nice separation between the two groups of points! By using the Petal Length and Petal Width, we could draw a line on this graph to separate the species easily. Now if we got a new flower, we could determine which species it is based upon if it lies above that line or below that line. Note that you can click the box of a species in the legend to hide those points.",
          "imgSrc": "blank"
        }
      ],
      "demoComp": "StaticAxisChart"
    },
    {
      "title": "Comparing Projections",
      "colorScheme": "dark",
      "subsections": [
        {
          "title": "Subsection 1",
          "body": "It's great that we have a way to differentiate between Setosa and Versicolor subtypes but remember that we have three different Iris subtypes that we need to be able to recognize. We can still use a similar approach to projecting our data into a 2D space and seeing how we can use a combination of features to determine the best way to separate incoming flowers into the correct subtypes.",
          "imgSrc": "blank"
        },
        {
          "title": "Subsection 2",
          "body": "Instead of only using the features Petal Length and Petal Width, let's consider all of the information that we collected so our efforts do not go to waste. We can try a wide variety of combinations to see what pairing of features might yield a way for us to draw two lines and separate each group of subtypes from the rest. Here we are only considering 2D comparisions but in real life applications, this can be generalized to many dimensions to find separation boundaries.",
          "imgSrc": "blank"
        },
        {
          "title": "Subsection 3",
          "body": "To use the demo below, select the values you want to plot on each axis by clicking the light blue box corresponding to the feature you are interested in. If you only want to compare two subtypes, click the box at the top with the color that corresponds to the subtype you want to exclude (you can reenable it by clicking again). Experiment with the possible pairings to find what is the best way to separate species! Note that when you choose the same variable on the same axis, you will consistently get a line as the resulting plot, why is that?",
          "imgSrc": "blank"
        }
      ],
      "demoComp": "SelectableAxisChart"
    },
    {
      "title": "Advanced Projections",
      "colorScheme": "light",
      "subsections": [
        {
          "title": "Subsection 1",
          "body": "Now we'll consider a more advanced technique to encourage separation amongst the subtypes that we are working on distinguishing. In order to separate data, we want to encourage the most variation between subtypes along each axis. Visually, this means that we would want the clusters to be located at spread apart locations on a single axis. For example, in the very first plot, the Setosa points are ranged from [1, 2] for Petal Length whereas Versicolor points are from [3, 5.5] which allows us to separate them easily.",
          "imgSrc": "blank"
        },
        {
          "title": "Subsection 2",
          "body": "One technique is to combine our features to create a new feature that maximizes such variation. This technique is known as Principle Component Analysis and it uses a combination of the original features to develop new features called Principle Components (PCs). This combination of features is referred to as a \"linear combination\" in linear algebra since individual features are scaled and added to create the PCs. These PCs are ranked in the order of variation they provide across all data points. Notice that when you plot them below, the first PC provides an axis where points from the same class are tightly packed but points from separate classes are much more spread out. We can then plot these new features to obtain better separation results than using our original features. In our example, we have just two PCs, but sometimes more than two PCs may have to be used.",
          "imgSrc": "blank"
        },
        {
          "title": "Subsection 3",
          "body": "Below, we see that the species are more separeted when we use the first two Principle Components to separate the data. Notice that the first principle component maximizes the difference between the blue cluster (Setosa) and the rest of the points. However, the second PC does not possess nearly as much variation between the classes, making it possible to learn an approximate but not entirely accurate separation boundary between the Vertosa and Virginica flowers. Do you think that the visualization here creates more separation? Why or why not?",
          "imgSrc": "blank"
        }
      ],
      "demoComp": "PCASelectableAxisChart"
    },
    {
      "title": "Interactive Clustering",
      "colorScheme": "dark",
      "subsections": [
        {
          "title": "Subsection 1",
          "body": "Now we will explore a different example where we have a bunch of data and all we know about that data is that there are two classes. We do not have any prior information about what observations belongs to which class. Like in our earlier classification example, the goal is to figure out a way to separate these points, but now we want our boundaries to separate this unlabeled data into two groups which are as distinctly different as possible.",
          "imgSrc": "blank"
        },
        {
          "title": "Subsection 2",
          "body": "As we saw previously, the problem of classification is directly dependent on finding boundaries which separate features as much as possible while keeping observations of the same class clustered together. The goal here is similar, but since we don't know the classes of observations, we have to see if we find a way to separate the data into clusters automatically. With the K-means algorithm, we are trying to figure out a way to find K different clusters in our data. In the example alluded to before, we could use the K-means algorithm with a K of 2 to separate the data into two clusters. But the question of how to determine these clusters is still ambiguous. The intuition is that within a cluster (i.e. among points of the same color), we want to minimize the distances between points of the same color but between clusters (i.e. comparing now to points of different colors), we want to maximizes distances to points of different colors. So, we want to figure out how to partition the points into clusters where the middle of each cluster is as close as possible to the points in the same cluster, while being as far as possible from the points in other clusters.",
          "imgSrc": "blank"
        },
        {
          "title": "Subsection 3",
          "body": "In order to develop a better understanding of how this algorithm works in practice, we've enabled the below demo to be entirely customizable. You can drag the dark circles of the corresponding datasets which represent the 'centers' and see how the classification of the points differs based on where the center is located! Notice that when you are dragging the center around, you are not truly performing the technique we described above called K-Means but rather just experimenting with different center locations - remember that we do not have labels! You can add points by clicking in any blank space while editing is enabled, you can change the proportion of points shown, and you can select to only see the custom points you added.",
          "imgSrc": "blank"
        }
      ],
      "demoComp": "InteractiveKMeans"
    },
    {
      "title": "K-Means",
      "colorScheme": "light",
      "subsections": [
        {
          "title": "Subsection 1",
          "body": "Now that you've tried out moving the clusters around, let's look at how the K-Means algorithm does this automatically. For this demo, we have restricted the customizability to only be the steps that the algorithm is at in its optimization process. Remember that the optimization here is two parts, we want to minimize the amount of distance between points within the same cluster and maximize the distance between points in different clusters. So each step changes the center of the two clusters such that there is an improvement in that metric. By looking at this in a step by step manner, we can visualize the process the algorithm takes to finally converge on the centers.",
          "imgSrc": "blank"
        },
        {
          "title": "Subsection 2",
          "body": "Remember that you can do this yourself in the plot above! As you drag the centers around, notice the data points may be recolored, they change color based on where the centers are repositioned to.",
          "imgSrc": "blank"
        },
        {
          "title": "Subsection 3",
          "body": "Initially, we have the K-Means algorithm in its first step, where there are 2 centers (since k=2) in a random spot on the chart - these are the darker colored points. They could start anywhere and the algorithm would still work. All the data points are colored according to which center they are closest to. For each iteration of K-Means, the centers are repositioned to the center of all the data points that belong to it. Each loop of K-Means involves repositioning each center once, and then repeating until they converge (stop moving).",
          "imgSrc": "blank"
        }
      ],
      "demoComp": "StepKMeans"
    }
  ]
}
