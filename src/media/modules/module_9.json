{
  "number": 9,
  "title": "Recognition as a Classification Problem",
  "sections": [
    {
      "title": "Why State Spaces?",
      "colorScheme": "dark",
      "subsections": [
        {
          "title": "Subsection 1",
          "body": "Often times, when we want to learn about a phenomena, whether that be the weather, sports, or business sales, we collect large amounts of data to extract information from. We collect this data about the specific characteristics that we can directly observe and write down, generally we collect two types of date: qualitative and quantitative data. Today our discussion will center around quantitative data.",
          "imgSrc": "blank"
        },
        {
          "title": "Subsection 2",
          "body": "Let's pretend we are florists and we want to learn about the characteristics of the flowers in our garden. We know we have three types of Iris flowers in our garden: Setosa, Versicolor, and Virginica flowers. We go through and collect some easily measurable information about all of the flowers, particularly, we collect information about the petal length and width as well as the sepal length and width. We are not concerned with which specific plant those measurements came from but we do want to track what subtype of Iris that plant is.",
          "imgSrc": "blank"
        },
        {
          "title": "Subsection 3",
          "body": "Fantastic! Now we have that data for every single one of our plants, each being an 'observation', with their corresponding characteristic values which we will refer to as 'features' from now on. We can easily combine the singular observations into a table of observations since each has the exact same features. Now, times are tough for the flower industry so we cannot hire someone to label each of our Iris flowers as the correct subtype when a new shipment comes in. Can you come up with a way to determine subtype just based on the table below? Note that you can click the triangle in the top right corner of the table to reveal the true subtypes. ",
          "imgSrc": "blank"
        }
      ],
      "demoComp": "RawDataTable"
    },
    {
      "title": "Projecting Data",
      "colorScheme": "light",
      "subsections": [
        {
          "title": "Subsection 1",
          "body": "It is certainly not easy to find a nice way to determine a subtype just from the data table of many observations and features! Another approach to understanding our data is to see what the distribution of our observations is for some combination of the features so we can see if flowers of the same subtype have similar values for some features.",
          "imgSrc": "blank"
        },
        {
          "title": "Subsection 2",
          "body": "A straightforward approach to this is to project our data into a 2D plot. You have certainly come across a 2D plot in other contexts but here we are plotting the values for individual features along each axis. So for a single data point, we are looking at x and y coordinates based on the Petal Length of the flower and the Petal Width of the flower on the x and y axes respectively. So a single flower can be represented as (Petal Length, Petal Width).",
          "imgSrc": "blank"
        },
        {
          "title": "Subsection 3",
          "body": "First, let's only compare the Setosa and Versicolor flowers. Since we remembered to write down what subtype each of our flowers/observations is our current garden, we can color our points based on that information. Here, the Setosa flowers are blue and the Versicolor flowers are red. We also see a nice separation between the two groups of points! By using the Petal Length and Petal Width, we could draw a line to separate the classes down from 1.8 on the y-axis down to 3.5 on the x-axis. Now if new flowers come in, we can say that the flower's coordinates lie above that line then it is a Versicolor and if the coordinates are below, it is a Setosa. Note that you can click the box of a class in the legend to hide the points belonging to that class.",
          "imgSrc": "blank"
        }
      ],
      "demoComp": "StaticAxisChart"
    },
    {
      "title": "Comparing Projections",
      "colorScheme": "dark",
      "subsections": [
        {
          "title": "Subsection 1",
          "body": "It's great that we have a way to differentiate between Setosa and Versicolor subtypes but remember that we have three different Iris subtypes that we need to be able to recognize. We can still use a similar approach to projecting our data into a 2D space and seeing how we can use a combination of features to determine the best way to separate incoming flowers into the correct subtypes.",
          "imgSrc": "blank"
        },
        {
          "title": "Subsection 2",
          "body": "Instead of only using the features Petal Length and Petal Width, let's consider all of the information that we collected so our efforts do not go to waste. We can try a wide variety of combinations to see what pairing of features might yield a way for us to draw two lines and separate each group of subtypes from the rest. Here we are only considering 2D comparisions but in real life applications, this can be generalized to many dimensions to find separation boundaries.",
          "imgSrc": "blank"
        },
        {
          "title": "Subsection 3",
          "body": "To use the demo below, select the values you want to plot on each axis by clicking the light blue box corresponding to the feature you are interested in. The plot will automatically resize and animate the plot to change to displaying the new pair of axes you have selected. If you only want to compare two subtypes, click the box at the top with the color that corresponds to the subtype you want to exclude (you can reenable it by clicking again). Experiment with the possible pairings to find what is the approximate best way to separate your subtypes! Note that when you choose the same variable on the same axis, you will consistently get a line as the resulting plot, why is that?",
          "imgSrc": "blank"
        }
      ],
      "demoComp": "SelectableAxisChart"
    },
    {
      "title": "Advanced Projections",
      "colorScheme": "light",
      "subsections": [
        {
          "title": "Subsection 1",
          "body": "Now we'll consider a more advanced technique to encourage separation amongst the subtypes that we are working on distinguishing. In order to separate data, we want to encourage the most variation between subtypes along each axis. Visually, this means that we would want the clusters to be located at spread apart locations on a single axis. For example, in the very first plot, the Setosa points are ranged from [1, 2] for Petal Length whereas Versicolor points are from [3, 5.5] which allows us to separate them easily.",
          "imgSrc": "blank"
        },
        {
          "title": "Subsection 2",
          "body": "One technique is to combine our features to create a new feature that maximizes such variation. This technique is known as Principle Component Analysis and it uses a *combination of the original features to develop new features called Principle Components (PCs)*. This combination of features is referred to as a *linear combination* in linear algebra since individual features are scaled and added to create the PCs. These PCs are ranked in the order of variation they provide across all data points. Notice that when you plot them below, the first PC provides an axis where points from the same class are tightly packed but points from separate classes are much more spread out. We can then plot these new features to obtain better separation results than using our original features, sometimes more than two PCs may have to be used.",
          "imgSrc": "blank"
        },
        {
          "title": "Subsection 3",
          "body": "Below, we see that this is the case when we attempt to use the first two Principle Components to separate the data. Notice that the first principle component maximizes the difference between the blue cluster (Setosa) and the rest of the points. However, the second PC does not possess nearly as much variation between the classes, making it possible to learn an approximate but not entirely accurate separation boundary (i.e. a single or set of line(s) we can draw between the two sets of points) for the Versicolor and Virginica classes. Note that you are not restricted to plotting PCs against other PCs in real life so feel free to do the below, but remember that since all the features can used to develop a PC you may be seeing some duplicate information! Do you think that the visualization here creates more separation? Why or why not?",
          "imgSrc": "blank"
        }
      ],
      "demoComp": "PCASelectableAxisChart"
    },
    {
      "title": "Interactive Clustering",
      "colorScheme": "dark",
      "subsections": [
        {
          "title": "Subsection 1",
          "body": "Now we will explore a different, contextless example where we have two classes of points that we want to label as the appropriate class. Here, we do not have any prior information about what point belongs to what class which begs the question: how can we classify new points when we do not know the classes of the original points? Instead of finding a separation boundary directly -  like we did above where we were able to find a set of lines that separated the two groups of points - we first have to determine which points belong to the same group!",
          "imgSrc": "blank"
        },
        {
          "title": "Subsection 2",
          "body": "As we saw previously, the problem of classification is directly dependent on distances between classes. In order to determine what those classes are in the first place, we can use a distance metric across all of our data points to find K centers (where K is our hunch about how many clusters there are) which are the means for that specfic class's data. So the center for a Setosa flower for example would be defined as the average Sepal length and width as well as the average Petal length and width of all Setosa flowers. But the question of how to determine these clusters is still a little ambiguous - the intuition is that within a cluster (i.e. among points of the same color), we want to minimize the distances between points of the same color but between clusters (i.e. comparing now to points of different colors), we want to maximizes distances to points of different colors. Ultimately, we want our centers to be such that they are at the mean of the points of the same color so that the difference from the center to each point of the same color is minimized. It all comes back to separation and distances!",
          "imgSrc": "blank"
        },
        {
          "title": "Subsection 3",
          "body": "In order to develop a better understanding of how this algorithm works in practice, we've enabled the below demo to be entirely customizable. You can drag the dark circles of the corresponding datasets which represent the 'centers' and see how the classification of the points differs based on where the center is located! Notice that when you are dragging the center around, you are not truly performing the technique we described above called K-Means but rather just experimenting with different center locations - remember that we do not have labels! Note that you can add points by clicking in any blank space while editing is enabled, you can change the proportion of points shown, and you can select to only see the custom points you added. Please do not click the center points, only drag them (we are ironing this bug out).",
          "imgSrc": "blank"
        }
      ],
      "demoComp": "InteractiveKMeans"
    },
    {
      "title": "K-Means",
      "colorScheme": "light",
      "subsections": [
        {
          "title": "Subsection 1",
          "body": "For this demo, we have restricted the customizability to only be the steps that the algorithm is at in its optimization process. Remember that the optimization here is two parts, we want to minimize the amount of distance between points within the same cluster and maximize the distance between points in different clusters. So each step changes the center of the two clusters such that there is an improvement in that metric. By looking at this in a step by step manner, we can visualize the 'gradient descent' process to the optimal location of the centers as the data points stay the same but the centers are updated to modify the points that are considered to be part of the same cluster.",
          "imgSrc": "blank"
        },
        {
          "title": "Subsection 2",
          "body": "Remember that you can do this yourself in the plot above! As you drag the centers around, notice the data points may be recolored, they change color based on where the centers are repositioned to. While we are not displaying the values for the distances within and between clusters, you can still take the logical steps to find the locations where these differences would be logically optimized and then compare this to the results of the algorithm by looking at what the output is of the plot below for the last iteration.",
          "imgSrc": "blank"
        },
        {
          "title": "Subsection 3",
          "body": "Initially, we have the kmeans algorithm in its first step, where there are 2 centers (since k=2) in a random spot on the chart - these are the darker colored points. All the data points are colored according to which center they are closest to. For each iteration of kmeans, the centers are repositioned to the center of all the data points that belong to it. Each loop of kmeans involves repositioning each center once, and then repeating until they converge (stop moving).",
          "imgSrc": "blank"
        }
      ],
      "demoComp": "StepKMeans"
    }
  ]
}
