
{
  "computer-vision": {
    "number": 8,
    "title": "Module 8: Visual Features for Computer Vision",
    "sections": [
      {
        "title": "What is Computer Vision?",
        "colorScheme": "dark",
        "subsections": [
          {
            "title": "",
            "body": "In order to extract valuable information from an image, computers must be able to process images and detect signals based on some criteria. As humans, we do this all the time - if we see an animal, we can identify it as a cat because it has fur, perked up ears, a small nose, and bright eyes. We extract features/traits of the objects in our view that allow us to identify those objects.",
            "imgSrc": "/blank.png"
          },
          {
            "title": "",
            "body": "Computer vision is conceptually not all that different! To identify features in an image, a small window called a *kernel* slides across the image and at every step, makes a calculation.",
            "imgSrc": "/blank.png"
          },
          {
            "title": "",
            "body": "Depending on what trait the kernel is supposed to capture, that calculation reveals whether that trait is present at the current location of the window or not. These calculations are then joined into a new image that represents the result of sliding that window across the original image.",
            "imgSrc": "/blank.png"
          }
        ]
      },
      {
        "title": "What is a Kernel?",
        "colorScheme": "light",
        "subsections": [
          {
            "title": "Subsection 1",
            "body": "Let's take a step back and define what an image is first. An image is made up of individual pixels which each have a value that represents the color, and those pixels are organized into a grid of values to give us the resulting image. However, the organization of the pixels is very particular! We can have the exact same pixel values in two grids, just in different orders and the image would be entirely unrecognizable.",
            "imgSrc": "/animation-1.gif"
          },
          {
            "title": "Subsection 2",
            "body": "Kernels are not all the different from images in that sense - we specify a much smaller grid of numbers that, based on their value and organization are capable of extracting a \"feature\" from the part of the image the kernel is currently sitting on. A feature might be the edge of an object in an image, it might be used to sharpen the details of an image, or it may be used to smooth an image.",
            "imgSrc": "/animation-2.gif"
          },
          {
            "title": "Subsection 3",
            "body": "The way this feature extraction occurs is by multiplying the corresponding parts of the kernel with the pixel value directly below it and summing all those products up — let's call that product P. We compute all the P terms by sliding this kernel across the image and repeating the math at every step. As we slide across the image, we stitch together the P terms to create a brand new image!",
            "imgSrc": "/animation-3.gif"
          }
        ]
      },
      {
        "title": "Gaussian Blur",
        "colorScheme": "dark",
        "subsections": [
          {
            "title": "Subsection 1",
            "body": "Intuitively, this kernel is used for blurring. This is useful to help amplify the overall image structure, when the picture is being passed into some sort of classification system to identify the image. The same way we don't recognize a cell phone by its serial number, an image processing system wants to focus on the big picture as well (sorry, not sorry).",
            "imgSrc": "/blank.png"
          },
          {
            "title": "Subsection 2",
            "body": "The definition of a Gaussian Blur Filter comes from the famous Gaussian/Normal/Bell curve as though it were projected into 3D. The values of the kernel are based on the density of (area under) the bell curve at the location of the current kernel entry. For example, the center of the kernel is the center of the bell curve where the density is the highest; therefore, the largest value of the kernel should be at the center. Conversely, values on the edges of the kernel should be relatively the smallest.",
            "imgSrc": "/blank.png"
          },
          {
            "title": "Subsection 3",
            "body": "We can control the change in the magnitude of the values from the center to the outsides of the kernel (spread of the bell curve) by modifying the standard deviation of the distribution using the σ. The last thing we want to make sure about the kernel is that the values sum up to approximately 1. The motivation for this is to avoid adding or subtracting \"energy\" to the image. We can do this by summing up all of the values and dividing every entry by the sum.",
            "imgSrc": "/blank.png"
          },
          {
            "title": "Subsection 3",
            "body": "This kernel extracts the importance of the current window with the center pixel of the window being the focus and surrounding pixels being increasingly less important as they get further from the center. Effectively, the outputted image is less bright where there are not high intensity pixels and vice versa. As a result, we achieve our initial goal of retaining the overall important structure of the image while masking the distracting details under the blur.",
            "imgSrc": "/blank.png"
          }
        ],
        "demoComp": "GaussianBlurDemo"
      },
      {
        "title": "Gabor Filter",
        "colorScheme": "light",
        "subsections": [
          {
            "title": "Subsection 1",
            "body": "The Gabor Filter is a bit more complicated in its definition but the features that it aims to extract are just as intuitive as the Gaussian Blur. Instead of wanting to extract the overall structure of the image, now we are interested in extracting portions of an image that follow a specific directional pattern. The canonical example of this is wanting to extract stripes on a zebra that are in a particular direction.",
            "imgSrc": "/blank.png"
          },
          {
            "title": "Subsection 2",
            "body": "The gist of how the filter works is that we can detect certain orientations by modifying our angle θ and specify the size of the window we slide across the image by modifying σ. We can also specify a frequency or magnitude for the directionality that assesses how strong the directionality is as a filter.",
            "imgSrc": "/blank.png"
          },
          {
            "title": "Subsection 3",
            "body": "The resulting image contains only the windows of the image that contain features that follow the directionality specified in the generation of the kernel. This can be highly effective when scanning images of text documents that also contain images where we only want to retain the text in the image. The reason for this is that images generally have lower frequency directional components as they are smoother relative to text.",
            "imgSrc": "/blank.png"
          }
        ],
        "demoComp": "GaborDemo"
      },
      {
        "title": "Difference of Gaussians",
        "colorScheme": "dark",
        "subsections": [
          {
            "title": "Subsection 1",
            "body": "The motivation behind the Difference of Gaussians technique is to detect edges in an image. This is particularly useful for segmenting objects from one another or creating a coloring book from pictures on your phone!",
            "imgSrc": "/blank.png"
          },
          {
            "title": "Subsection 2",
            "body": "A difference of Gaussians Kernel builds directly upon what we have already covered in Gaussian Blurring. We take two images processed using Gaussian Blur filters with different standard deviations and subtract them from each other. We want the image that we are subtracting from to have been processed with a Gaussian Blur filter that has a smaller standard deviation (less spread) than the image that we are subtracting.",
            "imgSrc": "/blank.png"
          },
          {
            "title": "Subsection 3",
            "body": "More intuitively, the reason that this computation works is because we extract the structurally important parts of the image using two Gaussian Blur filters initially. Naturally, the most structurally significant parts of objects in an image are the edges so when we subtract the two processed images, the resulting image is the information that lies between those two images which is the difference in the precision of the edges extracted.",
            "imgSrc": "/blank.png"
          }
        ],
        "demoComp": "DiffOfGaussian"
      }
    ]
  },
  "classification": {
    "number": 9,
    "title": "Module 9: Recognition as a Classification Problem",
    "sections": [
      {
        "title": "Why State Spaces?",
        "colorScheme": "dark",
        "subsections": [
          {
            "title": "Subsection 1",
            "body": "Often times, when we want to learn about a phenomena, whether that be the weather, sports, or business sales, we collect large amounts of data to extract information from. We collect this data about the specific characteristics that we can directly observe and write down, generally we collect two types of date: qualitative and quantitative data. Today our discussion will center around quantitative data.",
            "imgSrc": "/blank.png"
          },
          {
            "title": "Subsection 2",
            "body": "Let's pretend we are florists and we want to learn about the characteristics of the flowers in our garden. We know we have three types of Iris flowers in our garden: Setosa, Versicolor, and Virginica flowers. We go through and collect some easily measurable information about all of the flowers, particularly, we collect information about the petal length and width as well as the sepal length and width. We are not concerned with which specific plant those measurements came from but we do want to track what subtype of Iris that plant is.",
            "imgSrc": "/blank.png"
          },
          {
            "title": "Subsection 3",
            "body": "Fantastic! Now we have that data for every single one of our plants, each being an 'observation', with their corresponding characteristic values which we will refer to as 'features' from now on. We can easily combine the singular observations into a table of observations since each has the exact same features. Now, times are tough for the flower industry so we cannot hire someone to label each of our Iris flowers as the correct subtype when a new shipment comes in. Can you come up with a way to determine subtype just based on the table below?",
            "imgSrc": "/blank.png"
          }
        ],
        "demoComp": "RawDataTable"
      },
      {
        "title": "Projecting Data",
        "colorScheme": "light",
        "subsections": [
          {
            "title": "Subsection 1",
            "body": "It is certainly not easy to find a nice way to determine a subtype just from the data table of many observations and features! Another approach to understanding our data is to see what the distribution of our observations is for some combination of the features so we can see if flowers of the same subtype have similar values for some features.",
            "imgSrc": "/blank.png"
          },
          {
            "title": "Subsection 2",
            "body": "A straightforward approach to this is to project our data into a 2D plot. You have certainly come across a 2D plot in other contexts but here we are plotting the values for individual features along each axis. So for a single data point, we are looking at x and y coordinates based on the Petal Length of the flower and the Petal Width of the flower on the x and y axes respectively. So a single flower can be represented as (Petal Length, Petal Width).",
            "imgSrc": "/blank.png"
          },
          {
            "title": "Subsection 3",
            "body": "First, let's only compare the Setosa and Versicolor flowers. Since we remembered to write down what subtype each of our flowers/observations is our current garden, we can color our points based on that information. Here, the Setosa flowers are blue and the Versicolor flowers are red. We also see a nice separation between the two groups of points! By using the Petal Length and Petal Width, we could draw a line to separate the classes down from 1.8 on the y-axis down to 3.5 on the x-axis. Now if new flowers come in, we can say that the flower's coordinates lie above that line then it is a Versicolor and if the coordinates are below, it is a Setosa.",
            "imgSrc": "/blank.png"
          }
        ],
        "demoComp": "StaticAxisChart"
      },
      {
        "title": "Comparing Projections",
        "colorScheme": "dark",
        "subsections": [
          {
            "title": "Subsection 1",
            "body": "It's great that we have a way to differentiate between Setosa and Versicolor subtypes but remember that we have three different Iris subtypes that we need to be able to recognize. We can still use a similar approach to projecting our data into a 2D space and seeing how we can use a combination of features to determine the best way to separate incoming flowers into the correct subtypes.",
            "imgSrc": "/blank.png"
          },
          {
            "title": "Subsection 2",
            "body": "Instead of only using the features Petal Length and Petal Width, let's consider all of the information that we collected so our efforts do not go to waste. We can try a wide variety of combinations to see what pairing of features might yield a way for us to draw two lines and separate each group of subtypes from the rest. Here we are only considering 2D comparisions but in real life applications, this can be generalized to many dimensions to find separation boundaries.",
            "imgSrc": "/blank.png"
          },
          {
            "title": "Subsection 3",
            "body": "To use the demo below, select the values you want to plot on each axis by clicking the light blue box corresponding to the feature you are interested in. The plot will automatically resize and animate the plot to change to displaying the new pair of axes you have selected. If you only want to compare two subtypes, click the box at the top with the color that corresponds to the subtype you want to exclude (you can reenable it by clicking again). Experiment with the possible pairings to find what is the approximate best way to separate your subtypes!",
            "imgSrc": "/blank.png"
          }
        ],
        "demoComp": "SelectableAxisChart"
      },
      {
        "title": "Advanced Projections",
        "colorScheme": "light",
        "subsections": [
          {
            "title": "Subsection 1",
            "body": "Now we'll consider a more advanced technique to encourage separation amongst the subtypes that we are working on distinguishing. In order to separate data, we want to encourage the most variation between subtypes along each axis. Visually, this means that we would want the clusters to be located at spread apart locations on a single axis. For example, in the very first plot, the Setosa points are ranged from [1, 2] for Petal Length whereas Versicolor points are from [3, 5.5] which allows us to separate them easily.",
            "imgSrc": "/blank.png"
          },
          {
            "title": "Subsection 2",
            "body": "One technique is to combine our features to create a new feature that maximizes such variation. This technique is known as Principle Component Analysis and it uses a combination of the original features to develop new features called Principle Components (PCs). These PCs are ranked in the order of variation they provide across all data points. We can then plot these new features to obtain better separation results than using our original features, sometimes more than two PCs may have to be used.",
            "imgSrc": "/blank.png"
          },
          {
            "title": "Subsection 3",
            "body": "Below, we see that this is the case when we attempt to use the first two Principle Components to separate the data. Notice that the first principle component maximizes the difference between the blue cluster (Setosa) and the rest of the points. However, the second PC does not possess nearly as much variation between the classes, making it possible to learn an approximate but not entirely accurate separation boundary for the Versicolor and Virginica classes. Note that you are not restricted to plotting PCs against other PCs in real life so feel free to do the below but remember that since all the features can used to develop a PC you may be seeing some duplicate information!",
            "imgSrc": "/blank.png"
          }
        ],
        "demoComp": "PCASelectableAxisChart"
      },
      {
        "title": "Interactive Clustering",
        "colorScheme": "dark",
        "subsections": [
          {
            "title": "Subsection 1",
            "body": "Now we will explore a different, contextless example where we have two classes of points that we want to label as the appropriate class. Here, we do not have any prior information about what point belongs to what class which begs the question: how can we classify new points when we do not know the classes of the original points? Instead of finding a separation boundary directly like we did above, we first have to determine which points belong to the same group!",
            "imgSrc": "/blank.png"
          },
          {
            "title": "Subsection 2",
            "body": "As we saw previously, the problem of classification is directly dependent on distances between classes. In order to determine what those classes are in the first place, we can use a distance metric across all of our data points to find K centers (where K is our hunch about how many clusters there are) which are the means for that specfic class's data. So the center for a Setosa flower for example would be defined as the average Sepal/Petal length and width of all Setosa flowers. But the question of how to determine these clusters is still a little ambiguous - the intuition is that within a cluster, we want to minimizes the distances between points but between clusters, we want to maximizes distances. It all comes back to separation and distances!",
            "imgSrc": "/blank.png"
          },
          {
            "title": "Subsection 3",
            "body": "In order to develop a better understanding of how this algorithm works in practice, we've enabled the below demo to be entirely customizable. You can drag the dark circles of the corresponding datasets which represent the 'centers' and see how the classification of the points differs based on where the center is located! Notice that when you are dragging the center around, you are not truly performing the technique we described above called K-Means but rather just experimenting with different center locations - remember that we do not have labels! Note that you can add points by clicking in any white space while editing is enabled, you can change the proportion of points shown, and you can select to only see the custom points you added.",
            "imgSrc": "/blank.png"
          }
        ],
        "demoComp": "InteractiveKMeans"
      },
      {
        "title": "K-Means",
        "colorScheme": "light",
        "subsections": [
          {
            "title": "Subsection 1",
            "body": "For this demo, we have restricted the customizability to only be the steps that the algorithm is at in its optimization process. Remember that the optimization here is two parts, we want to minimize the amount of distance between points within the same cluster and maximize the distance between points in different clusters. So each step changes the center of the two clusters such that there is an improvement in that metric. By looking at this in a step by step manner, we can visualize the 'gradient descent' process to the optimal location of the centers as the data points stay the same but the centers are updated to modify the points that are considered to be part of the same cluster.",
            "imgSrc": "/blank.png"
          },
          {
            "title": "Subsection 2",
            "body": "Remember that you can do this yourself in the plot above! As you drag the centers around, notice the data points may be recolored, they change color based on where the centers are repositioned to. While we are not displaying the values for the distances within and between clusters, you can still take the logical steps to find the locations where these differences would be logically optimized and then compare this to the results of the algorithm by looking at what the output is of the plot below for the last iteration",
            "imgSrc": "/blank.png"
          },
          {
            "title": "Subsection 3",
            "body": "Initially, we have the kmeans algorithm in its first step, where there are 2 centers (since k=2) in a random spot on the chart - these are the darker colored points. All the data points are colored according to which center they are closest to. For each iteration of kmeans, the centers are repositioned to the center of all the data points that belong to it. Each loop of kmeans involves repositioning each center once, and then repeating until they converge (stop moving).",
            "imgSrc": "/blank.png"
          }
        ],
        "demoComp": "StepKMeans"
      }
    ]
  },
  "perceptrons": {
    "number": 10,
    "title": "Module 10: Introduction to Machine Learning: Perceptrons",
    "sections": [
      {
        "title": "Section 1",
        "colorScheme": "dark",
        "subsections": [
          {
            "title": "Subsection 1",
            "body": "lorem ipsum",
            "imgSrc": "mod10.png"
          },
          {
            "title": "Subsection 2",
            "body": "lorem ipsum",
            "imgSrc": "mod10.png"
          },
          {
            "title": "Subsection 3",
            "body": "lorem ipsum",
            "imgSrc": "mod10.png"
          }
        ]
      },
      {
        "title": "Section 2",
        "colorScheme": "light",
        "subsections": [
          {
            "title": "Subsection 1",
            "body": "lorem ipsum",
            "imgSrc": "mod10.png"
          },
          {
            "title": "Subsection 2",
            "body": "lorem ipsum",
            "imgSrc": "mod10.png"
          },
          {
            "title": "Subsection 3",
            "body": "lorem ipsum",
            "imgSrc": "mod10.png"
          }
        ]
      },
      {
        "title": "Section 3",
        "colorScheme": "dark",
        "subsections": [
          {
            "title": "Subsection 1",
            "body": "lorem ipsum",
            "imgSrc": "mod10.png"
          },
          {
            "title": "Subsection 2",
            "body": "lorem ipsum",
            "imgSrc": "mod10.png"
          },
          {
            "title": "Subsection 3",
            "body": "lorem ipsum",
            "imgSrc": "mod10.png"
          }
        ]
      }
    ]
  },
  "neural-nets": {
    "number": 11,
    "title": "Module 11: Multi-layer Perceptrons & Backpropagation",
    "sections": [
      {
        "title": "Section 1",
        "colorScheme": "dark",
        "subsections": [
          {
            "title": "Subsection 1",
            "body": "lorem ipsum",
            "imgSrc": "mod11.png"
          },
          {
            "title": "Subsection 2",
            "body": "lorem ipsum",
            "imgSrc": "mod11.png"
          },
          {
            "title": "Subsection 3",
            "body": "lorem ipsum",
            "imgSrc": "mod11.png"
          }
        ]
      },
      {
        "title": "Section 2",
        "colorScheme": "light",
        "subsections": [
          {
            "title": "Subsection 1",
            "body": "lorem ipsum",
            "imgSrc": "mod11.png"
          },
          {
            "title": "Subsection 2",
            "body": "lorem ipsum",
            "imgSrc": "mod11.png"
          },
          {
            "title": "Subsection 3",
            "body": "lorem ipsum",
            "imgSrc": "mod11.png"
          }
        ]
      },
      {
        "title": "Section 3",
        "colorScheme": "dark",
        "subsections": [
          {
            "title": "Subsection 1",
            "body": "lorem ipsum",
            "imgSrc": "mod11.png"
          },
          {
            "title": "Subsection 2",
            "body": "lorem ipsum",
            "imgSrc": "mod11.png"
          },
          {
            "title": "Subsection 3",
            "body": "lorem ipsum",
            "imgSrc": "mod11.png"
          }
        ]
      }
    ]
  }
}